{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/130 Train Loss: 1.914 Train Acc: 25.55% Test Loss: 1.638 Test Acc: 36.23%\n",
      "Epoch 2/130 Train Loss: 1.625 Train Acc: 37.56% Test Loss: 1.548 Test Acc: 43.04%\n",
      "Epoch 3/130 Train Loss: 1.385 Train Acc: 49.38% Test Loss: 1.471 Test Acc: 55.43%\n",
      "Epoch 4/130 Train Loss: 1.187 Train Acc: 57.77% Test Loss: 1.305 Test Acc: 63.02%\n",
      "Epoch 5/130 Train Loss: 1.057 Train Acc: 63.27% Test Loss: 1.248 Test Acc: 65.73%\n",
      "Epoch 6/130 Train Loss: 0.964 Train Acc: 66.56% Test Loss: 1.002 Test Acc: 70.81%\n",
      "Epoch 7/130 Train Loss: 0.892 Train Acc: 69.32% Test Loss: 0.787 Test Acc: 73.29%\n",
      "Epoch 8/130 Train Loss: 0.831 Train Acc: 71.63% Test Loss: 0.793 Test Acc: 75.80%\n",
      "Epoch 9/130 Train Loss: 0.781 Train Acc: 73.33% Test Loss: 0.798 Test Acc: 77.21%\n",
      "Epoch 10/130 Train Loss: 0.748 Train Acc: 74.76% Test Loss: 0.729 Test Acc: 78.46%\n",
      "Epoch 11/130 Train Loss: 0.703 Train Acc: 76.28% Test Loss: 0.671 Test Acc: 80.71%\n",
      "Epoch 12/130 Train Loss: 0.663 Train Acc: 77.73% Test Loss: 0.644 Test Acc: 79.80%\n",
      "Epoch 13/130 Train Loss: 0.633 Train Acc: 78.96% Test Loss: 0.546 Test Acc: 82.75%\n",
      "Epoch 14/130 Train Loss: 0.604 Train Acc: 79.80% Test Loss: 0.564 Test Acc: 82.90%\n",
      "Epoch 15/130 Train Loss: 0.579 Train Acc: 80.78% Test Loss: 0.554 Test Acc: 83.08%\n",
      "Epoch 16/130 Train Loss: 0.557 Train Acc: 81.51% Test Loss: 0.564 Test Acc: 82.12%\n",
      "Epoch 17/130 Train Loss: 0.536 Train Acc: 82.31% Test Loss: 0.524 Test Acc: 83.74%\n",
      "Epoch 18/130 Train Loss: 0.518 Train Acc: 82.78% Test Loss: 0.470 Test Acc: 84.57%\n",
      "Epoch 19/130 Train Loss: 0.505 Train Acc: 83.28% Test Loss: 0.515 Test Acc: 84.00%\n",
      "Epoch 20/130 Train Loss: 0.485 Train Acc: 83.95% Test Loss: 0.454 Test Acc: 85.07%\n",
      "Epoch 21/130 Train Loss: 0.470 Train Acc: 84.10% Test Loss: 0.441 Test Acc: 85.40%\n",
      "Epoch 22/130 Train Loss: 0.455 Train Acc: 84.91% Test Loss: 0.466 Test Acc: 85.44%\n",
      "Epoch 23/130 Train Loss: 0.444 Train Acc: 85.18% Test Loss: 0.475 Test Acc: 85.16%\n",
      "Epoch 24/130 Train Loss: 0.435 Train Acc: 85.28% Test Loss: 0.436 Test Acc: 86.09%\n",
      "Epoch 25/130 Train Loss: 0.420 Train Acc: 85.90% Test Loss: 0.546 Test Acc: 84.41%\n",
      "Epoch 26/130 Train Loss: 0.410 Train Acc: 86.29% Test Loss: 0.457 Test Acc: 85.94%\n",
      "Epoch 27/130 Train Loss: 0.404 Train Acc: 86.67% Test Loss: 0.442 Test Acc: 86.29%\n",
      "Epoch 28/130 Train Loss: 0.393 Train Acc: 86.84% Test Loss: 0.459 Test Acc: 86.73%\n",
      "Epoch 29/130 Train Loss: 0.379 Train Acc: 87.37% Test Loss: 0.521 Test Acc: 85.23%\n",
      "Epoch 30/130 Train Loss: 0.372 Train Acc: 87.43% Test Loss: 0.434 Test Acc: 86.89%\n",
      "Epoch 31/130 Train Loss: 0.366 Train Acc: 87.74% Test Loss: 0.415 Test Acc: 86.98%\n",
      "Epoch 32/130 Train Loss: 0.348 Train Acc: 88.30% Test Loss: 0.398 Test Acc: 86.97%\n",
      "Epoch 33/130 Train Loss: 0.346 Train Acc: 88.36% Test Loss: 0.431 Test Acc: 86.52%\n",
      "Epoch 34/130 Train Loss: 0.344 Train Acc: 88.56% Test Loss: 0.409 Test Acc: 87.25%\n",
      "Epoch 35/130 Train Loss: 0.331 Train Acc: 88.97% Test Loss: 0.419 Test Acc: 87.28%\n",
      "Epoch 36/130 Train Loss: 0.331 Train Acc: 88.92% Test Loss: 0.430 Test Acc: 86.95%\n",
      "Epoch 37/130 Train Loss: 0.327 Train Acc: 89.00% Test Loss: 0.435 Test Acc: 87.08%\n",
      "Epoch 38/130 Train Loss: 0.318 Train Acc: 89.26% Test Loss: 0.385 Test Acc: 88.20%\n",
      "Epoch 39/130 Train Loss: 0.315 Train Acc: 89.42% Test Loss: 0.395 Test Acc: 87.84%\n",
      "Epoch 40/130 Train Loss: 0.306 Train Acc: 89.78% Test Loss: 0.395 Test Acc: 87.72%\n",
      "Epoch 41/130 Train Loss: 0.300 Train Acc: 89.92% Test Loss: 0.453 Test Acc: 86.88%\n",
      "Epoch 42/130 Train Loss: 0.298 Train Acc: 89.98% Test Loss: 0.452 Test Acc: 87.06%\n",
      "Epoch 43/130 Train Loss: 0.290 Train Acc: 90.24% Test Loss: 0.414 Test Acc: 87.68%\n",
      "Epoch 44/130 Train Loss: 0.286 Train Acc: 90.37% Test Loss: 0.393 Test Acc: 87.75%\n",
      "Epoch 45/130 Train Loss: 0.278 Train Acc: 90.67% Test Loss: 0.433 Test Acc: 87.43%\n",
      "Epoch 46/130 Train Loss: 0.279 Train Acc: 90.66% Test Loss: 0.376 Test Acc: 88.41%\n",
      "Epoch 47/130 Train Loss: 0.270 Train Acc: 90.92% Test Loss: 0.434 Test Acc: 87.13%\n",
      "Epoch 48/130 Train Loss: 0.271 Train Acc: 90.86% Test Loss: 0.400 Test Acc: 87.66%\n",
      "Epoch 49/130 Train Loss: 0.259 Train Acc: 91.30% Test Loss: 0.398 Test Acc: 87.74%\n",
      "Epoch 50/130 Train Loss: 0.260 Train Acc: 91.29% Test Loss: 0.396 Test Acc: 87.76%\n",
      "Epoch 51/130 Train Loss: 0.261 Train Acc: 91.27% Test Loss: 0.406 Test Acc: 87.65%\n",
      "Epoch 52/130 Train Loss: 0.253 Train Acc: 91.52% Test Loss: 0.399 Test Acc: 88.19%\n",
      "Epoch 53/130 Train Loss: 0.251 Train Acc: 91.57% Test Loss: 0.392 Test Acc: 88.33%\n",
      "Epoch 54/130 Train Loss: 0.249 Train Acc: 91.72% Test Loss: 0.425 Test Acc: 87.80%\n",
      "Epoch 55/130 Train Loss: 0.245 Train Acc: 91.81% Test Loss: 0.414 Test Acc: 87.55%\n",
      "Epoch 56/130 Train Loss: 0.236 Train Acc: 92.05% Test Loss: 0.388 Test Acc: 88.58%\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 57/130 Train Loss: 0.236 Train Acc: 92.06% Test Loss: 0.400 Test Acc: 88.74%\n",
      "Epoch 58/130 Train Loss: 0.184 Train Acc: 93.68% Test Loss: 0.348 Test Acc: 89.85%\n",
      "Epoch 59/130 Train Loss: 0.165 Train Acc: 94.39% Test Loss: 0.358 Test Acc: 89.64%\n",
      "Epoch 60/130 Train Loss: 0.156 Train Acc: 94.72% Test Loss: 0.348 Test Acc: 90.16%\n",
      "Epoch 61/130 Train Loss: 0.150 Train Acc: 94.92% Test Loss: 0.355 Test Acc: 89.96%\n",
      "Epoch 62/130 Train Loss: 0.144 Train Acc: 95.07% Test Loss: 0.348 Test Acc: 90.18%\n",
      "Epoch 63/130 Train Loss: 0.144 Train Acc: 95.08% Test Loss: 0.343 Test Acc: 90.43%\n",
      "Epoch 64/130 Train Loss: 0.138 Train Acc: 95.35% Test Loss: 0.340 Test Acc: 90.36%\n",
      "Epoch 65/130 Train Loss: 0.139 Train Acc: 95.25% Test Loss: 0.350 Test Acc: 90.23%\n",
      "Epoch 66/130 Train Loss: 0.133 Train Acc: 95.49% Test Loss: 0.342 Test Acc: 90.37%\n",
      "Epoch 67/130 Train Loss: 0.131 Train Acc: 95.50% Test Loss: 0.351 Test Acc: 90.29%\n",
      "Epoch 68/130 Train Loss: 0.129 Train Acc: 95.57% Test Loss: 0.349 Test Acc: 90.18%\n",
      "Epoch 69/130 Train Loss: 0.131 Train Acc: 95.48% Test Loss: 0.353 Test Acc: 90.22%\n",
      "Epoch 70/130 Train Loss: 0.127 Train Acc: 95.62% Test Loss: 0.350 Test Acc: 90.39%\n",
      "Epoch 71/130 Train Loss: 0.126 Train Acc: 95.63% Test Loss: 0.361 Test Acc: 89.94%\n",
      "Epoch 72/130 Train Loss: 0.125 Train Acc: 95.72% Test Loss: 0.357 Test Acc: 90.39%\n",
      "Epoch 73/130 Train Loss: 0.123 Train Acc: 95.78% Test Loss: 0.355 Test Acc: 90.48%\n",
      "Epoch 74/130 Train Loss: 0.118 Train Acc: 96.01% Test Loss: 0.358 Test Acc: 90.33%\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 75/130 Train Loss: 0.118 Train Acc: 95.89% Test Loss: 0.358 Test Acc: 90.15%\n",
      "Epoch 76/130 Train Loss: 0.116 Train Acc: 95.96% Test Loss: 0.350 Test Acc: 90.41%\n",
      "Epoch 77/130 Train Loss: 0.114 Train Acc: 96.16% Test Loss: 0.356 Test Acc: 90.36%\n",
      "Epoch 78/130 Train Loss: 0.116 Train Acc: 96.08% Test Loss: 0.353 Test Acc: 90.39%\n",
      "Epoch 79/130 Train Loss: 0.115 Train Acc: 96.06% Test Loss: 0.368 Test Acc: 90.37%\n",
      "Epoch 80/130 Train Loss: 0.115 Train Acc: 96.02% Test Loss: 0.353 Test Acc: 90.39%\n",
      "Epoch 81/130 Train Loss: 0.113 Train Acc: 96.00% Test Loss: 0.361 Test Acc: 90.39%\n",
      "Epoch 82/130 Train Loss: 0.112 Train Acc: 96.13% Test Loss: 0.364 Test Acc: 90.16%\n",
      "Epoch 83/130 Train Loss: 0.111 Train Acc: 96.18% Test Loss: 0.357 Test Acc: 90.40%\n",
      "Epoch 84/130 Train Loss: 0.110 Train Acc: 96.31% Test Loss: 0.352 Test Acc: 90.36%\n",
      "Epoch 85/130 Train Loss: 0.110 Train Acc: 96.24% Test Loss: 0.363 Test Acc: 90.15%\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 86/130 Train Loss: 0.112 Train Acc: 96.09% Test Loss: 0.363 Test Acc: 90.21%\n",
      "Epoch 87/130 Train Loss: 0.107 Train Acc: 96.35% Test Loss: 0.363 Test Acc: 90.50%\n",
      "Epoch 88/130 Train Loss: 0.109 Train Acc: 96.29% Test Loss: 0.357 Test Acc: 90.37%\n",
      "Epoch 89/130 Train Loss: 0.111 Train Acc: 96.05% Test Loss: 0.346 Test Acc: 90.34%\n",
      "Epoch 90/130 Train Loss: 0.109 Train Acc: 96.19% Test Loss: 0.360 Test Acc: 90.52%\n",
      "Epoch 91/130 Train Loss: 0.111 Train Acc: 96.15% Test Loss: 0.366 Test Acc: 90.32%\n",
      "Epoch 92/130 Train Loss: 0.110 Train Acc: 96.18% Test Loss: 0.362 Test Acc: 90.25%\n",
      "Epoch 93/130 Train Loss: 0.110 Train Acc: 96.22% Test Loss: 0.356 Test Acc: 90.36%\n",
      "Epoch 94/130 Train Loss: 0.110 Train Acc: 96.23% Test Loss: 0.356 Test Acc: 90.61%\n",
      "Epoch 95/130 Train Loss: 0.111 Train Acc: 96.23% Test Loss: 0.349 Test Acc: 90.55%\n",
      "Epoch 96/130 Train Loss: 0.111 Train Acc: 96.19% Test Loss: 0.353 Test Acc: 90.37%\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 97/130 Train Loss: 0.112 Train Acc: 96.12% Test Loss: 0.347 Test Acc: 90.40%\n",
      "Epoch 98/130 Train Loss: 0.112 Train Acc: 96.18% Test Loss: 0.349 Test Acc: 90.41%\n",
      "Epoch 99/130 Train Loss: 0.110 Train Acc: 96.23% Test Loss: 0.354 Test Acc: 90.47%\n",
      "Epoch 100/130 Train Loss: 0.110 Train Acc: 96.24% Test Loss: 0.348 Test Acc: 90.46%\n",
      "Epoch 101/130 Train Loss: 0.111 Train Acc: 96.08% Test Loss: 0.354 Test Acc: 90.39%\n",
      "Epoch 102/130 Train Loss: 0.108 Train Acc: 96.27% Test Loss: 0.364 Test Acc: 90.46%\n",
      "Epoch 103/130 Train Loss: 0.112 Train Acc: 96.15% Test Loss: 0.358 Test Acc: 90.16%\n",
      "Epoch 104/130 Train Loss: 0.112 Train Acc: 96.17% Test Loss: 0.361 Test Acc: 90.20%\n",
      "Epoch 105/130 Train Loss: 0.108 Train Acc: 96.27% Test Loss: 0.358 Test Acc: 90.31%\n",
      "Epoch 106/130 Train Loss: 0.110 Train Acc: 96.16% Test Loss: 0.350 Test Acc: 90.49%\n",
      "Epoch 107/130 Train Loss: 0.110 Train Acc: 96.21% Test Loss: 0.357 Test Acc: 90.63%\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 108/130 Train Loss: 0.111 Train Acc: 96.26% Test Loss: 0.355 Test Acc: 90.43%\n",
      "Epoch 109/130 Train Loss: 0.112 Train Acc: 96.17% Test Loss: 0.352 Test Acc: 90.47%\n",
      "Epoch 110/130 Train Loss: 0.108 Train Acc: 96.23% Test Loss: 0.356 Test Acc: 90.43%\n",
      "Epoch 111/130 Train Loss: 0.109 Train Acc: 96.30% Test Loss: 0.352 Test Acc: 90.35%\n",
      "Epoch 112/130 Train Loss: 0.109 Train Acc: 96.25% Test Loss: 0.356 Test Acc: 90.45%\n",
      "Epoch 113/130 Train Loss: 0.108 Train Acc: 96.23% Test Loss: 0.348 Test Acc: 90.45%\n",
      "Epoch 114/130 Train Loss: 0.111 Train Acc: 96.23% Test Loss: 0.358 Test Acc: 90.52%\n",
      "Epoch 115/130 Train Loss: 0.113 Train Acc: 96.17% Test Loss: 0.351 Test Acc: 90.32%\n",
      "Epoch 116/130 Train Loss: 0.113 Train Acc: 96.12% Test Loss: 0.366 Test Acc: 90.08%\n",
      "Epoch 117/130 Train Loss: 0.111 Train Acc: 96.14% Test Loss: 0.359 Test Acc: 90.35%\n",
      "Epoch 118/130 Train Loss: 0.112 Train Acc: 96.18% Test Loss: 0.365 Test Acc: 90.32%\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 119/130 Train Loss: 0.110 Train Acc: 96.28% Test Loss: 0.355 Test Acc: 90.41%\n",
      "Epoch 120/130 Train Loss: 0.111 Train Acc: 96.20% Test Loss: 0.359 Test Acc: 90.43%\n",
      "Epoch 121/130 Train Loss: 0.112 Train Acc: 96.15% Test Loss: 0.357 Test Acc: 90.45%\n",
      "Epoch 122/130 Train Loss: 0.107 Train Acc: 96.29% Test Loss: 0.349 Test Acc: 90.49%\n",
      "Epoch 123/130 Train Loss: 0.110 Train Acc: 96.22% Test Loss: 0.354 Test Acc: 90.43%\n",
      "Epoch 124/130 Train Loss: 0.110 Train Acc: 96.19% Test Loss: 0.359 Test Acc: 90.44%\n",
      "Epoch 125/130 Train Loss: 0.108 Train Acc: 96.19% Test Loss: 0.361 Test Acc: 90.32%\n",
      "Epoch 126/130 Train Loss: 0.110 Train Acc: 96.18% Test Loss: 0.355 Test Acc: 90.41%\n",
      "Epoch 127/130 Train Loss: 0.105 Train Acc: 96.42% Test Loss: 0.354 Test Acc: 90.57%\n",
      "Epoch 128/130 Train Loss: 0.109 Train Acc: 96.20% Test Loss: 0.353 Test Acc: 90.36%\n",
      "Epoch 129/130 Train Loss: 0.109 Train Acc: 96.23% Test Loss: 0.355 Test Acc: 90.43%\n",
      "Epoch 130/130 Train Loss: 0.110 Train Acc: 96.11% Test Loss: 0.352 Test Acc: 90.46%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Task 1 - Dataset loaders\n",
    "def load_cifar_10(batch_size):\n",
    "    train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), # Data augmentations for training data, along with normalization values (mean, std) for CIFAR-10 across 3 channels\n",
    "                                            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                                            transforms.RandomRotation(10),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    \n",
    "    test_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                          transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "    cifar_train = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=train_transforms, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=test_transforms, download=True)  # change download to true\n",
    "\n",
    "    return data.DataLoader(cifar_train, batch_size, shuffle=True, num_workers=2), data.DataLoader(cifar_test, batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "batch_size = 32\n",
    "train_iter, test_iter = load_cifar_10(batch_size)\n",
    "\n",
    "# Task 2 - Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.drop = nn.Dropout1d(0.5)\n",
    "\n",
    "        # Block 1\n",
    "        # Block 1 (a) MLP Instance variables\n",
    "        self.block_1_fc = nn.Linear(3, 3)\n",
    "        self.block_1_fc_bn = nn.BatchNorm1d(3)\n",
    "        self.block_1_a_relu = nn.ReLU()\n",
    "\n",
    "        # K convolutional layers for block 1 (3)\n",
    "        self.block_1_convolutions = nn.ModuleList([nn.Conv2d(3, 128, kernel_size=3, padding=1) for _ in range(3)])\n",
    "        self.block_1_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.block_1_sequential = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # 16 x 16            \n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # 8 x 8\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        ) \n",
    "\n",
    "        # BLock 2\n",
    "        # Block 2 (a) MLP Instance variables\n",
    "        self.block_2_fc_bn = nn.BatchNorm1d(128)\n",
    "        self.block_2_fc_bn_1 = nn.BatchNorm1d(64)\n",
    "        self.block_2_fc_bn_2 = nn.BatchNorm1d(3)\n",
    "\n",
    "        self.block_2_fc = nn.Linear(256, 128)\n",
    "        self.block_2_fc_1 = nn.Linear(128, 64)\n",
    "        self.block_2_fc_2 = nn.Linear(64, 3)\n",
    "\n",
    "        self.block_2_a_relu = nn.ReLU()\n",
    "\n",
    "        # K convolutional layers for block 2 (3)\n",
    "        self.block_2_convolutions = nn.ModuleList([nn.Conv2d(256, 384, kernel_size=3, padding=1) for _ in range(3)])\n",
    "        self.block_2_bn = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.block_2_sequential = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # 4 x 4            \n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # 2 x 2\n",
    "            nn.Conv2d(384, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        ) \n",
    "\n",
    "        # Classifier MLP variables\n",
    "        self.bn_c1 = nn.BatchNorm1d(256)\n",
    "        self.bn_c2 = nn.BatchNorm1d(128)\n",
    "        self.bn_c3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc_1 = nn.Linear(512, 256)\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.class_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Block 1\n",
    "        #  Linear to determine a\n",
    "        #  [end shape] a.shape = [32, 3], k = 3\n",
    "        #  a = [[a1,a2,...,ak],\n",
    "        #       [a1,a2,...,ak]]\n",
    "\n",
    "        a = x.mean(dim=[2, 3]) # Taking avg across H,W dimensions [Spatial Average Pool] [32, 3]\n",
    "        \n",
    "        a = self.block_1_fc(a) \n",
    "        a = self.block_1_fc_bn(a)\n",
    "        a = self.block_1_a_relu(a)\n",
    "        a = self.drop(a)\n",
    "\n",
    "        # convk(x)\n",
    "        conv_outputs = [self.block_1_bn(conv(x)) for conv in self.block_1_convolutions]       \n",
    "\n",
    "        # a1 * conv1(x) + ... + ak * convk(x) \n",
    "        out = sum([a[i, j].item() * conv_outputs[j] for i in range(len(a)) for j in range(len(a[i]))])\n",
    "\n",
    "        out = self.block_1_sequential(out)\n",
    "\n",
    "\n",
    "        #  Block 2\n",
    "        #  MLP to determine a\n",
    "        #  [end shape] a_1.shape = [32, 3], k = 3\n",
    "        #  a = [[a1,a2,...,a3],\n",
    "        #       [a1,a2,...,a3]]\n",
    "\n",
    "        a_1 = out.mean(dim=[2, 3]) # [32, 256]\n",
    "\n",
    "        a_1 = self.block_2_fc(a_1)\n",
    "        a_1 = self.block_2_fc_bn(a_1)\n",
    "        a_1 = self.block_2_a_relu(a_1)\n",
    "\n",
    "        a_1 = self.block_2_fc_1(a_1)\n",
    "        a_1 = self.block_2_fc_bn_1(a_1)\n",
    "        a_1 = self.block_2_a_relu(a_1)\n",
    "\n",
    "        a_1 = self.block_2_fc_2(a_1)\n",
    "        a_1 = self.block_2_fc_bn_2(a_1)\n",
    "        a_1 = self.block_2_a_relu(a_1)\n",
    "\n",
    "        # convk(x)\n",
    "        conv_outputs = [self.block_2_bn(conv(out)) for conv in self.block_2_convolutions]\n",
    "\n",
    "        # a1 * conv1(x) + ... + ak * convk(x) \n",
    "        out_1 = sum([a_1[i, j].item() * conv_outputs[j] for i in range(len(a_1)) for j in range(len(a_1[i]))])\n",
    "\n",
    "        out_1 = self.block_2_sequential(out_1)\n",
    "\n",
    "        # Classifier MLP\n",
    "        out_1 = out_1.mean(dim=[2, 3]) # [32, 512]\n",
    "        out_1 = self.fc_1(out_1)\n",
    "        out_1 = self.bn_c1(out_1)\n",
    "        out_1 = self.class_relu(out_1)\n",
    "\n",
    "        out_1 = self.fc_2(out_1)\n",
    "        out_1 = self.bn_c2(out_1)\n",
    "        out_1 = self.class_relu(out_1)\n",
    "\n",
    "        out_1 = self.fc_3(out_1)\n",
    "        out_1 = self.bn_c3(out_1)\n",
    "        out_1 = self.class_relu(out_1)\n",
    "\n",
    "        out_1 = self.fc_4(out_1)\n",
    "        return out_1\n",
    "    \n",
    "# Task 4 - Training script\n",
    "def train(net, train_iter, test_iter, epochs=130):\n",
    "    # Set up optimizer and loss function\n",
    "    \n",
    "    # Task 3 - Loss and Optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, min_lr=0) # Scheduler to lower lr by factor if test loss stagnates for 10 epochs (patience)\n",
    "\n",
    "    # Move the model to the GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train\n",
    "        net.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "\n",
    "            # if (i + 1) % 100 == 0:\n",
    "            #     print(f\"Epoch [{epoch + 1}/{epochs}] -> Batch [{i + 1}/{50000 // batch_size}]\")\n",
    "\n",
    "            # Move to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            loss = loss_function(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Training stats\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = y_hat.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        train_loss /= len(train_iter)\n",
    "\n",
    "\n",
    "        # Evaluate\n",
    "        net.eval()\n",
    "        test_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        # No need to back prop for inference\n",
    "        with torch.no_grad():\n",
    "            for (X, y) in test_iter:\n",
    "\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = net(X)\n",
    "                loss = loss_function(y_hat, y)\n",
    "\n",
    "                # Test stats\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = y_hat.max(1)\n",
    "                total += y.size(0)\n",
    "                correct += predicted.eq(y).sum().item()\n",
    "\n",
    "        test_acc = 100. * correct / total\n",
    "        test_loss /= len(test_iter)\n",
    "            \n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'model_state_dict': net.state_dict(),\n",
    "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #     'scheduler_state_dict': scheduler.state_dict(),\n",
    "        #     'train_loss': train_loss,\n",
    "        #     'train_acc': train_acc,\n",
    "        #     'test_acc': test_acc,\n",
    "        #     'test_loss': test_loss\n",
    "        #     }   , f'Test/checkpoint{epoch + 1}.pth')\n",
    "        \n",
    "        # Print epoch stats\n",
    "        print(f'Epoch {epoch + 1}/{epochs} Train Loss: {train_loss:.3f} Train Acc: {train_acc:.2f}% Test Loss: {test_loss:.3f} Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "\n",
    "net = Model()\n",
    "train(net, train_iter, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
